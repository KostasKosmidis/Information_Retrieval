{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Crowler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "from bs4 import BeautifulSoup\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "   \"\"\"\n",
    "    Συλλέγει περιεχόμενο από άρθρα της Wikipedia.\n",
    "\n",
    "    Παράμετροι:\n",
    "    topics: Λίστα θεμάτων προς συλλογή.\n",
    "    num_articles: Αριθμός παραγράφων που θα συλλεχθούν.\n",
    "\n",
    "    Επιστρέφει:\n",
    "    Λίστα με δεδομένα άρθρων.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_wikipedia_articles(topics, num_articles=5):\n",
    "    dataset = []\n",
    "    for topic in topics:\n",
    "        url = f\"https://en.wikipedia.org/wiki/{topic.replace(' ', '_')}\"\n",
    "        print(f\"Συλλογή άρθρου: {url}\")\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            paragraphs = soup.find_all('p')\n",
    "            content = \" \".join([para.text for para in paragraphs[:num_articles]])\n",
    "            dataset.append({\n",
    "                \"title\": topic,\n",
    "                \"content\": content,\n",
    "                \"url\": url\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Σφάλμα συλλογής για το θέμα: {topic}\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "    Αποθηκεύει τα δεδομένα άρθρων σε αρχείο JSON.\n",
    "\n",
    "    Παράμετροι:\n",
    "    dataset: Δεδομένα προς αποθήκευση.\n",
    "    file_path: Διαδρομή αρχείου.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(dataset, file_path='dataset.json'):\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(dataset, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Δεδομένα αποθηκεύτηκαν στο {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "    Συλλέγει άρθρα από προκαθορισμένες σελίδες της Wikipedia.\n",
    "\n",
    "    Επιστρέφει:\n",
    "    Λίστα με δεδομένα άρθρων.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_articles():\n",
    "    urls = [\n",
    "        \"https://en.wikipedia.org/wiki/Rock_music\",\n",
    "        \"https://en.wikipedia.org/wiki/Pop_music\",\n",
    "        \"https://en.wikipedia.org/wiki/Heavy_metal_music\",\n",
    "        \"https://en.wikipedia.org/wiki/Metallica\",\n",
    "        \"https://en.wikipedia.org/wiki/Linkin_Park\",\n",
    "        \"https://en.wikipedia.org/wiki/Avenged_Sevenfold\",\n",
    "        \"https://en.wikipedia.org/wiki/Jazz\",\n",
    "        \"https://en.wikipedia.org/wiki/Classical_music\",\n",
    "        \"https://en.wikipedia.org/wiki/Guitar\",\n",
    "        \"https://en.wikipedia.org/wiki/Violin\",\n",
    "        \"https://en.wikipedia.org/wiki/Clarinet\"\n",
    "    ]\n",
    "    articles = []\n",
    "\n",
    "    for url in urls:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        title = soup.find('h1').text\n",
    "        content = ' '.join([p.text for p in soup.find_all('p')])\n",
    "        articles.append({\"title\": title, \"content\": content, \"url\": url})\n",
    "        print(f\"Συλλογή άρθρου: {url}\")\n",
    "\n",
    "    return articles"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
